{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Walkthrough\n",
    "\n",
    "Here we will walk through the ETL process used in `etl.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load modules\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import configparser\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.sql.functions import hour, year, month, dayofmonth, weekofyear, dayofweek\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "# Set AWS key and secret key\n",
    "KEY = config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "SECRET = config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config.get('AWS','AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment to unzip the zipped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip song and log data\n",
    "#with zipfile.ZipFile('data/log-data.zip', 'r') as zip_ref:\n",
    "#    zip_ref.extractall('unzip_data/log_data')\n",
    "\n",
    "#with zipfile.ZipFile('data/song-data.zip', 'r') as zip_ref:\n",
    "#    zip_ref.extractall('unzip_data/song_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "#### Load Song Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create spark session\n",
    "spark = SparkSession.builder\\\n",
    "                    .config('spark.jars.packages','org.apache.hadoop:hadoop-aws:2.7.0')\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load song data\n",
    "song_data = spark.read.json('unzip_data/song_data/song_data/A/*/*/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check shcema\n",
    "song_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|  artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|ARDR4AC1187FB371A1|           null|                 |            null|Montserrat Caball...|511.16363|        1|SOBAYLL12A8C138AF9|Sono andati? Fing...|   0|\n",
      "|AREBBGV1187FB523D2|           null|      Houston, TX|            null|Mike Jones (Featu...|173.66159|        1|SOOLYAZ12A6701F4A6|Laws Patrolling (...|   0|\n",
      "|ARMAC4T1187FB3FA4C|       40.82624|Morris Plains, NJ|       -74.47995|The Dillinger Esc...|207.77751|        1|SOBBUGU12A8C13E95D|Setting Fire to S...|2004|\n",
      "|ARPBNLO1187FB3D52F|       40.71455|     New York, NY|       -74.00712|            Tiny Tim| 43.36281|        1|SOAOIBZ12AB01815BE|I Hold Your Hand ...|2000|\n",
      "|ARDNS031187B9924F0|       32.67828|          Georgia|       -83.22295|          Tim Wilson|186.48771|        1|SONYPOM12A8C13B2D7|I Think My Wife I...|2005|\n",
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check data\n",
    "song_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load song data\n",
    "log_data = spark.read.json('unzip_data/log_data/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get start time\n",
    "log_data = log_data.withColumn('start_time', F.from_unixtime(log_data['ts']/1000).cast(\"timestamp\")).filter(log_data['page']=='NextSong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Schema\n",
    "log_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+-------------------+\n",
      "|     artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|                song|status|           ts|           userAgent|userId|         start_time|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+-------------------+\n",
      "|   Harmonia|Logged In|     Ryan|     M|            0|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|       Sehr kosmisch|   200|1542241826796|\"Mozilla/5.0 (X11...|    26|2018-11-15 00:30:26|\n",
      "|The Prodigy|Logged In|     Ryan|     M|            1|   Smith|260.07465| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|     The Big Gundown|   200|1542242481796|\"Mozilla/5.0 (X11...|    26|2018-11-15 00:41:21|\n",
      "|      Train|Logged In|     Ryan|     M|            2|   Smith|205.45261| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|            Marry Me|   200|1542242741796|\"Mozilla/5.0 (X11...|    26|2018-11-15 00:45:41|\n",
      "|Sony Wonder|Logged In|   Samuel|     M|            0|Gonzalez|218.06975| free|Houston-The Woodl...|   PUT|NextSong|1.540492941796E12|      597|           Blackbird|   200|1542253449796|\"Mozilla/5.0 (Mac...|    61|2018-11-15 03:44:09|\n",
      "|  Van Halen|Logged In|    Tegan|     F|            2|  Levine|289.38404| paid|Portland-South Po...|   PUT|NextSong|1.540794356796E12|      602|Best Of Both Worl...|   200|1542260935796|\"Mozilla/5.0 (Mac...|    80|2018-11-15 05:48:55|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check data\n",
    "log_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time table\n",
    "Time table contains: start time, hour, day, week, month, year, weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get start time\n",
    "time_table = log_data.select(['start_time'])\n",
    "\n",
    "# Convert from start time to hour/day/week/month/year/weekday\n",
    "time_table = time_table.select(\n",
    "    'start_time',\n",
    "    hour(\"start_time\").alias('hour'),\n",
    "    dayofmonth(\"start_time\").alias('day'),\n",
    "    weekofyear(\"start_time\").alias('week'),\n",
    "    month(\"start_time\").alias('month'),\n",
    "    year(\"start_time\").alias('year'), \n",
    "    dayofweek(\"start_time\").alias('weekday')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|         start_time|hour|day|week|month|year|weekday|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-15 00:30:26|   0| 15|  46|   11|2018|      5|\n",
      "|2018-11-15 00:41:21|   0| 15|  46|   11|2018|      5|\n",
      "|2018-11-15 00:45:41|   0| 15|  46|   11|2018|      5|\n",
      "|2018-11-15 03:44:09|   3| 15|  46|   11|2018|      5|\n",
      "|2018-11-15 05:48:55|   5| 15|  46|   11|2018|      5|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+\n",
      "|summary|month|  year|\n",
      "+-------+-----+------+\n",
      "|  count| 6820|  6820|\n",
      "|   mean| 11.0|2018.0|\n",
      "| stddev|  0.0|   0.0|\n",
      "|    min|   11|  2018|\n",
      "|    max|   11|  2018|\n",
      "+-------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.describe('month','year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User table\n",
    "User table contains :user id, first name, last name, gender, and level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table = log_data.select(['userId','firstName','lastName','gender','level'])\\\n",
    "    .dropDuplicates(subset=['userId']).sort('lastName').where(log_data['userId']!= '')\n",
    "\n",
    "# Change userid type from string to integer\n",
    "user_table = user_table.withColumn(\"userId\", user_table[\"userId\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+------+-----+\n",
      "|userId|firstName|lastName|gender|level|\n",
      "+------+---------+--------+------+-----+\n",
      "|    66|    Kevin|Arellano|     M| free|\n",
      "|    34|   Evelin|   Ayala|     F| free|\n",
      "|    99|      Ann|   Banks|     F| free|\n",
      "|   100|    Adler| Barrera|     M| free|\n",
      "|    42|   Harper| Barrett|     M| paid|\n",
      "+------+---------+--------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Songs table\n",
    "Songs table contains: song id, title, artist id, year, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_table = song_data.select(['song_id','title','artist_id','year','duration']).dropDuplicates(subset=['song_id']).sort('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOFFKZS12AB017F194|A Higher Place (A...|ARBEBBY1187B9B43DB|1994|236.17261|\n",
      "|SODREIN12A58A7F2E5|A Whiter Shade Of...|ARLTWXK1187FB5A3F8|   0|326.00771|\n",
      "|SOXVLOJ12AB0189215|     Amor De Cabaret|ARKRRTF1187B9984DA|   0|177.47546|\n",
      "|SORAMLE12AB017C8B0|      Auguri Cha Cha|ARHHO3O1187B989413|   0|191.84281|\n",
      "|SOZHPGD12A8C1394FE|     Baby Come To Me|AR9AWNF1187B9AB0B4|   0|236.93016|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artist table\n",
    "Artist table contains: artist id, name, location, lattitude, longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_table = song_data.select(['artist_id','artist_name','artist_location','artist_latitude','artist_longitude']).dropDuplicates(subset=['artist_id']).sort('artist_name')\n",
    "artist_table = artist_table.selectExpr('artist_id as artist_id','artist_name as name',\\\n",
    "                                            'artist_location as location','artist_latitude as latitude',\\\n",
    "                                            'artist_longitude as longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+---------------+--------+---------+\n",
      "|         artist_id|           name|       location|latitude|longitude|\n",
      "+------------------+---------------+---------------+--------+---------+\n",
      "|AR558FS1187FB45658|        40 Grit|               |    null|     null|\n",
      "|AR7G5I41187FB4CE6C|       Adam Ant|London, England|    null|     null|\n",
      "|ARI3BMM1187FB4255E|   Alice Stuart|     Washington| 38.8991|  -77.029|\n",
      "|ARL7K851187B99ACD2|      Andy Andy|               |    null|     null|\n",
      "|AR3JMC51187B9AE49D|Backstreet Boys|    Orlando, FL|28.53823|-81.37739|\n",
      "+------------------+---------------+---------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artist_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artist_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song plays table\n",
    "Songplays table contains: songplay_id, start time, user id, level, song id, artist id, session, location, user agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from log data\n",
    "songplays_table = log_data.select(['start_time','userId','level','song','sessionId','location','userAgent','length']).filter(log_data['page']=='NextSong')\n",
    "\n",
    "#get song, songid, and artistid to join\n",
    "song_artist_table = song_data.select(['title','song_id','artist_id','duration']).dropDuplicates(subset=['song_id'])\n",
    "\n",
    "# Create songplay_id column\n",
    "songplays_table = songplays_table.withColumn(\"songplay_id\", F.monotonically_increasing_id())\n",
    "\n",
    "#join songid and artistid\n",
    "songplays_table = songplays_table.join(song_artist_table,\\\n",
    "                                      songplays_table.song == song_artist_table.title,\\\n",
    "                                      how='left')\n",
    "\n",
    "#drop ts song and title columns\n",
    "songplays_table = songplays_table.drop('title')\n",
    "\n",
    "# Change userid type from string to integer\n",
    "songplays_table = songplays_table.withColumn(\"userId\", songplays_table[\"userId\"].cast(IntegerType()))\n",
    "\n",
    "# create month column\n",
    "songplays_table = songplays_table.withColumn(\"month\",\\\n",
    "                                             month(songplays_table[\"start_time\"]).alias('month'))   \n",
    "# create year column\n",
    "songplays_table = songplays_table.withColumn(\"year\",\\\n",
    "                                             year(songplays_table[\"start_time\"]).alias('year'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+-----+--------------------+---------+--------------------+--------------------+---------+-----------+-------+---------+--------+-----+----+\n",
      "|         start_time|userId|level|                song|sessionId|            location|           userAgent|   length|songplay_id|song_id|artist_id|duration|month|year|\n",
      "+-------------------+------+-----+--------------------+---------+--------------------+--------------------+---------+-----------+-------+---------+--------+-----+----+\n",
      "|2018-11-15 00:30:26|    26| free|       Sehr kosmisch|      583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|655.77751|          0|   null|     null|    null|   11|2018|\n",
      "|2018-11-15 00:41:21|    26| free|     The Big Gundown|      583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|260.07465|          1|   null|     null|    null|   11|2018|\n",
      "|2018-11-15 00:45:41|    26| free|            Marry Me|      583|San Jose-Sunnyval...|\"Mozilla/5.0 (X11...|205.45261|          2|   null|     null|    null|   11|2018|\n",
      "|2018-11-15 03:44:09|    61| free|           Blackbird|      597|Houston-The Woodl...|\"Mozilla/5.0 (Mac...|218.06975|          3|   null|     null|    null|   11|2018|\n",
      "|2018-11-15 05:48:55|    80| paid|Best Of Both Worl...|      602|Portland-South Po...|\"Mozilla/5.0 (Mac...|289.38404|          4|   null|     null|    null|   11|2018|\n",
      "+-------------------+------+-----+--------------------+---------+--------------------+--------------------+---------+-----------+-------+---------+--------+-----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- songplay_id: long (nullable = false)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Method\n",
    "The method below uses the SQL to create the table matching song title, artist, and duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data.createOrReplaceTempView(\"events_sql\")\n",
    "time_table.createOrReplaceTempView(\"time_sql\")\n",
    "songs_table.createOrReplaceTempView(\"song_sql\")\n",
    "artist_table.createOrReplaceTempView(\"artist_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table_test = spark.sql(\"\"\"\n",
    "SELECT \n",
    "e.start_time, e.userId, e.level,\n",
    "s.song_id, s.artist_id, e.sessionId,\n",
    "e.userAgent, t.year, t.month\n",
    "FROM events_sql e\n",
    "JOIN song_sql s ON e.song = s.title AND e.length = s.duration\n",
    "JOIN artist_sql a ON e.artist = a.name AND a.artist_id = s.artist_id\n",
    "JOIN time_sql t ON e.start_time = t.start_time\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table_test.createOrReplaceTempView(\"songplays_sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count(1): bigint]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT count(*)\n",
    "FROM songplays_sql\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = \"s3a://udacity-datalakes-project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o201.parquet.\n: java.io.IOException: Bucket udacity-datalakes-project does not exist\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:298)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:424)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:524)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e6221a2b30ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msongs_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'artist_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_data\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'songs_table'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o201.parquet.\n: java.io.IOException: Bucket udacity-datalakes-project does not exist\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:298)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2669)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:424)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:524)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "songs_table.write.mode(\"overwrite\").partitionBy('year','artist_id').parquet(output_data+'songs_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_table.write.mode(\"overwrite\").parquet(output_data+'artist_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_table.write.mode(\"overwrite\").parquet(output_data+'user_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.write.mode(\"overwrite\").partitionBy('year','month').parquet(output_data+'songplays_table')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table.write.mode(\"overwrite\").partitionBy('year','month').parquet(output_data+'time_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
